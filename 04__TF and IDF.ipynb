{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82adf2a3",
   "metadata": {},
   "source": [
    "## # TF (Term Frequency) and IDF (Inverse Document Frequency):\n",
    "\n",
    "**`TF = No. of repitions of a Term in the Sentence/Total no. of Sentences`**\n",
    "\n",
    "**`IDF = log(No. of Sentences/No. of Sentences containing that t=Term )`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331bbb46",
   "metadata": {},
   "source": [
    "Using these techniques, our emphasis is on bringing some semantic meaning to the words represented in numerical form.\n",
    "\n",
    "**Key Intuition**: The key intuition motivating **TF-IDF** is the importance of a term is inversely related to its frequency across documents.\n",
    "\n",
    "* **`TF`** gives us information on how often a term appears in a document/sentence.\n",
    "\n",
    "\n",
    "* **`IDF`** gives us information about the **relative rarity** of a term in a collection of documents/sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35ecad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c332013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(30000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 30 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1bb9a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Turn data into actionable business insights.\\nThe AWS Data Science team uses the tools our cloud platform provides to unify data preparation, machine learning, and model deployment. We scale the abilities and resources of our customers by delivering advanced functionality for data visualization, feature engineering, model interpretability, and low-latency deployment. Our culture of data-driven decision making requires advanced sales technologies that are timely, accurate, and actionable.\\n\\nAs part of the AWS Data Science team, you’ll discover and solve real-world problems by analyzing large amounts of business data, defining new metrics and business cases, designing simulations and experiments, creating models, and collaborating with colleagues. You’ll bring with you a strong quantitative background and thrive in an environment that leverages statistics, machine learning, operations research, econometrics, and business analysis. And in return, you’ll have the chance to work on some of the world’s largest and diverse datasets.\\n\\nLearn more about Amazon’s approach to customer-obsessed science on the Amazon Science website, which features the latest news and research from scientists across the company. It’s where you can find information about the conferences we sponsor, the institutions we collaborate with, our awards program, career opportunities, challenges, and more. For the latest updates, subscribe to the monthly newsletter, and follow Amazon Science on LinkedIn, Twitter, Facebook, Instagram, and YouTube.\\n\\nInterested in AWS? Start here\\nWe’re always glad to connect with talented people. Tell us a bit about what you want to do and we’ll keep you posted on relevant roles and what we’re building at AWS. '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para = \"\"\"Turn data into actionable business insights.\n",
    "The AWS Data Science team uses the tools our cloud platform provides to unify data preparation, machine learning, and model deployment. We scale the abilities and resources of our customers by delivering advanced functionality for data visualization, feature engineering, model interpretability, and low-latency deployment. Our culture of data-driven decision making requires advanced sales technologies that are timely, accurate, and actionable.\n",
    "\n",
    "As part of the AWS Data Science team, you’ll discover and solve real-world problems by analyzing large amounts of business data, defining new metrics and business cases, designing simulations and experiments, creating models, and collaborating with colleagues. You’ll bring with you a strong quantitative background and thrive in an environment that leverages statistics, machine learning, operations research, econometrics, and business analysis. And in return, you’ll have the chance to work on some of the world’s largest and diverse datasets.\n",
    "\n",
    "Learn more about Amazon’s approach to customer-obsessed science on the Amazon Science website, which features the latest news and research from scientists across the company. It’s where you can find information about the conferences we sponsor, the institutions we collaborate with, our awards program, career opportunities, challenges, and more. For the latest updates, subscribe to the monthly newsletter, and follow Amazon Science on LinkedIn, Twitter, Facebook, Instagram, and YouTube.\n",
    "\n",
    "Interested in AWS? Start here\n",
    "We’re always glad to connect with talented people. Tell us a bit about what you want to do and we’ll keep you posted on relevant roles and what we’re building at AWS. \"\"\"\n",
    "\n",
    "para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "164dc73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization\n",
    "\n",
    "sentences = nltk.sent_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01f5a4f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<WordNetLemmatizer>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create an object for Lemmatizing\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13885ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step i. CLean the text\n",
    "\n",
    "corpus = []  # Cleaned sentences\n",
    "\n",
    "# Lowering the words\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-z]', ' ', sentences[i])  # Replace all chars apart from alphabets w white space\n",
    "    review = review.lower()\n",
    "    cleaned_words = nltk.word_tokenize(review)\n",
    "    \n",
    "    # Removing Stop Words followed by lemmatizing\n",
    "    cleaned_words = [lemma.lemmatize(i) for i in cleaned_words if cleaned_words not in stopwords.words('english')]\n",
    "    cleaned_sent = \" \".join(cleaned_words)\n",
    "#     print(cleaned_words)\n",
    "#     print(\"--\")\n",
    "    corpus.append(cleaned_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c6195c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn data into actionable business insights.\n",
      "turn data into actionable business insight \n",
      "--\n",
      "The AWS Data Science team uses the tools our cloud platform provides to unify data preparation, machine learning, and model deployment.\n",
      "the aws data science team us the tool our cloud platform provides to unify data preparation machine learning and model deployment \n",
      "--\n",
      "We scale the abilities and resources of our customers by delivering advanced functionality for data visualization, feature engineering, model interpretability, and low-latency deployment.\n",
      "we scale the ability and resource of our customer by delivering advanced functionality for data visualization feature engineering model interpretability and low latency deployment \n",
      "--\n",
      "Our culture of data-driven decision making requires advanced sales technologies that are timely, accurate, and actionable.\n",
      "our culture of data driven decision making requires advanced sale technology that are timely accurate and actionable \n",
      "--\n",
      "As part of the AWS Data Science team, you’ll discover and solve real-world problems by analyzing large amounts of business data, defining new metrics and business cases, designing simulations and experiments, creating models, and collaborating with colleagues.\n",
      "a part of the aws data science team you ll discover and solve real world problem by analyzing large amount of business data defining new metric and business case designing simulation and experiment creating model and collaborating with colleague \n",
      "--\n",
      "You’ll bring with you a strong quantitative background and thrive in an environment that leverages statistics, machine learning, operations research, econometrics, and business analysis.\n",
      "you ll bring with you a strong quantitative background and thrive in an environment that leverage statistic machine learning operation research econometrics and business analysis \n",
      "--\n",
      "And in return, you’ll have the chance to work on some of the world’s largest and diverse datasets.\n",
      "and in return you ll have the chance to work on some of the world s largest and diverse datasets \n",
      "--\n",
      "Learn more about Amazon’s approach to customer-obsessed science on the Amazon Science website, which features the latest news and research from scientists across the company.\n",
      "learn more about amazon s approach to customer obsessed science on the amazon science website which feature the latest news and research from scientist across the company \n",
      "--\n",
      "It’s where you can find information about the conferences we sponsor, the institutions we collaborate with, our awards program, career opportunities, challenges, and more.\n",
      "it s where you can find information about the conference we sponsor the institution we collaborate with our award program career opportunity challenge and more \n",
      "--\n",
      "For the latest updates, subscribe to the monthly newsletter, and follow Amazon Science on LinkedIn, Twitter, Facebook, Instagram, and YouTube.\n",
      "for the latest update subscribe to the monthly newsletter and follow amazon science on linkedin twitter facebook instagram and youtube \n",
      "--\n",
      "Interested in AWS?\n",
      "interested in aws \n",
      "--\n",
      "Start here\n",
      "We’re always glad to connect with talented people.\n",
      "start here we re always glad to connect with talented people \n",
      "--\n",
      "Tell us a bit about what you want to do and we’ll keep you posted on relevant roles and what we’re building at AWS.\n",
      "tell u a bit about what you want to do and we ll keep you posted on relevant role and what we re building at aws \n",
      "--\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Let's compare the original sentences against the cleaned ones\n",
    "\n",
    "[print(f'{i}\\n{j}', \"\\n--\") for i, j in zip(sentences, corpus)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998cee3e",
   "metadata": {},
   "source": [
    "### # Creating the TF-IDF model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d96c504",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "X = tfidf_vec.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf942c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.396865</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.236662</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.204088</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.163051</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.269669</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.232553</td>\n",
       "      <td>0.232553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.17667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.121719</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.152354</td>\n",
       "      <td>0.110785</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.227176</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156516</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.284911</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.27413</td>\n",
       "      <td>0.236400</td>\n",
       "      <td>0.171899</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160568</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.209973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.362146</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.209973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.209973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.171560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.309132</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.224347</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.154566</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.140681</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.220167</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.255306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.332902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.229357</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.229357</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.165833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.298813</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.433714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.271970</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13 rows × 155 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \\\n",
       "0   0.000000  0.000000  0.000000  0.000000  0.396865  0.000000  0.000000   \n",
       "1   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2   0.236662  0.000000  0.000000  0.000000  0.000000  0.204088  0.000000   \n",
       "3   0.000000  0.000000  0.269669  0.000000  0.232553  0.232553  0.000000   \n",
       "4   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "5   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "6   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "7   0.000000  0.160568  0.000000  0.209973  0.000000  0.000000  0.000000   \n",
       "8   0.000000  0.171560  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "9   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "10  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "11  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.332902   \n",
       "12  0.000000  0.165833  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         7        8         9    ...       145       146       147       148  \\\n",
       "0   0.000000  0.00000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "1   0.000000  0.00000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "2   0.000000  0.00000  0.000000  ...  0.163051  0.000000  0.000000  0.000000   \n",
       "3   0.000000  0.00000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "4   0.000000  0.17667  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "5   0.000000  0.00000  0.227176  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "6   0.000000  0.00000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "7   0.362146  0.00000  0.000000  ...  0.000000  0.209973  0.000000  0.000000   \n",
       "8   0.000000  0.00000  0.000000  ...  0.309132  0.000000  0.000000  0.224347   \n",
       "9   0.220167  0.00000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "10  0.000000  0.00000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "11  0.000000  0.00000  0.000000  ...  0.229357  0.000000  0.000000  0.000000   \n",
       "12  0.000000  0.00000  0.000000  ...  0.298813  0.000000  0.433714  0.000000   \n",
       "\n",
       "         149       150      151       152       153       154  \n",
       "0   0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  \n",
       "1   0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  \n",
       "2   0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  \n",
       "3   0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  \n",
       "4   0.000000  0.121719  0.00000  0.152354  0.110785  0.000000  \n",
       "5   0.000000  0.156516  0.00000  0.000000  0.284911  0.000000  \n",
       "6   0.000000  0.000000  0.27413  0.236400  0.171899  0.000000  \n",
       "7   0.209973  0.000000  0.00000  0.000000  0.000000  0.000000  \n",
       "8   0.000000  0.154566  0.00000  0.000000  0.140681  0.000000  \n",
       "9   0.000000  0.000000  0.00000  0.000000  0.000000  0.255306  \n",
       "10  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  \n",
       "11  0.000000  0.229357  0.00000  0.000000  0.000000  0.000000  \n",
       "12  0.000000  0.000000  0.00000  0.000000  0.271970  0.000000  \n",
       "\n",
       "[13 rows x 155 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our TF-IDF vectorized features\n",
    "\n",
    "X = X.toarray()\n",
    "X = pd.DataFrame(X)\n",
    "X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
